## Import the Iris dataset from scikit-learn
#from sklearn.datasets import load_iris
#import matplotlib.pyplot as plt
#
## Load the Iris dataset
#iris = load_iris()
#
## Create X and Y variables to hold features and response column
#iris_X, iris_Y = iris.data, iris.target
#
## The names of the flowers we are trying to predict
#iris.target_names
#
## Look at the names of the features
#iris.feature_names
#
## For labelling
#label_dict = {i: k for i, k in enumerate(iris.target_names)}
#
#def plot(X, Y, title, x_label, y_label):
#    plt.figure(figsize=(10, 6))  # Set figure size
#    for label, marker, color in zip(range(3), ('^', 's', 'o'), ('blue', 'red', 'green')):
#        plt.scatter(X[Y == label, 0], X[Y == label, 1],
#                    color=color, alpha=0.5,
#                    label=label_dict[label],
#                    marker=marker)
#    plt.xlabel(x_label)
#    plt.ylabel(y_label)
#    leg = plt.legend(loc='upper right', fancybox=True)
#    leg.get_frame().set_alpha(0.5)
#    plt.title(title)
#    plt.grid(True)  # Add grid for better readability
#    plt.show()  # Show the plot
#
## Call the plot function
#plot(iris_X, iris_Y, "Original Iris Data", "Sepal Length (cm)", "Sepal Width (cm)")

#--PCA Manual: Covariance matrix (code)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
iris_X = iris.data

# Calculate the mean vector
mean_vector = iris_X.mean(axis=0)
print("Mean Vector:", mean_vector)

# Calculate the covariance matrix
cov_mat = np.cov((iris_X - mean_vector).T)
print("Covariance Matrix Shape:", cov_mat.shape)

# Calculate the eigenvectors and eigenvalues of the covariance matrix
eig_val_cov, eig_vec_cov = np.linalg.eig(cov_mat)

# Print the eigenvectors and corresponding eigenvalues in order of descending eigenvalues
for i in range(len(eig_val_cov)):
    eigvec_cov = eig_vec_cov[:, i]
    print("Eigen Vector {}: {} with Eigenvalue {}".format(i + 1, eigvec_cov, eig_val_cov[i]))
    print(30 * "-")

# The percentage of the variance captured by each eigenvalue
variance_ratio = eig_val_cov / eig_val_cov.sum()
print("Variance Ratio:", variance_ratio)

# Scree Plot
plt.figure(figsize=(8, 5))
plt.plot(np.cumsum(variance_ratio), marker='o')
plt.title('Scree Plot')
plt.xlabel('Principal Component (k)')
plt.ylabel('% of Variance Explained <= k')
plt.grid()
plt.show()

# Store top two eigenvectors in a variable
top_2_eigenvectors = eig_vec_cov[:, :2]  # Corrected to select the first two eigenvectors
print("Top 2 Eigenvectors:\n", top_2_eigenvectors)

# Transform data from shape (150,4) to (150,2)
new_data = np.dot(iris_X, top_2_eigenvectors)
print("Transformed Data (first 5 rows):\n", new_data[:5])

# Function to plot PCA results
def plot(X, Y, title, x_label, y_label):
    plt.figure(figsize=(8, 6))
    for label, marker, color in zip(range(3), ('^', 's', 'o'), ('blue', 'red', 'green')):
        plt.scatter(X[Y == label, 0], X[Y == label, 1],
                    color=color, alpha=0.5,
                    label=iris.target_names[label],
                    marker=marker)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.title(title)
    plt.legend()
    plt.grid()
    plt.show()

# Call the plot function to visualize the PCA result
plot(new_data, iris.target, "Iris Data Transformation with PCA", 'PCA 1', 'PCA 2')


